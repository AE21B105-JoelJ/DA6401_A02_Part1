{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DA6401 - Assignment 02 (P1)\n",
    "        This notebook contains the source code written for this assignment which will be later transfered to a python scripy on successful passage of testing and checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "import lightning as L\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to give the activation function\n",
    "def return_activation_function(activation : str = \"ReLU\"):\n",
    "    possible_activations = [\"ReLU\", \"Mish\", \"GELU\", \"SELU\", \"SiLU\", \"LeakyReLU\" ]\n",
    "    # Assertion to be made for the activations possible\n",
    "    assert activation in possible_activations, f\"activation not in {possible_activations}\"\n",
    "\n",
    "    if activation == \"ReLU\":\n",
    "        return nn.ReLU()\n",
    "    elif activation == \"GELU\":\n",
    "        return nn.GELU()\n",
    "    elif activation == \"SiLU\":\n",
    "        return nn.SiLU()\n",
    "    elif activation == \"SELU\":\n",
    "        return nn.SELU()\n",
    "    elif activation == \"Mish\":\n",
    "        return nn.Mish()\n",
    "    else:\n",
    "        return nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lightning_CNN(L.LightningModule):\n",
    "    def __init__(self, no_of_conv = 5, activation_list : List = None, num_filters_list : List = None, filter_sizes : List = None, Batch_norm = False, n_classes = 10, input_size = 200,\n",
    "                 dropout : float = 0.3, max_pool_filter_size : List = None, max_pool_stride : List = None,dense_layer_neurons : int = 1024, dense_layer_activation : str = \"ReLU\" ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assertion of the passed hyperparameters\n",
    "        assert len(activation_list) == no_of_conv, \"The number of convolution layers and activations do not match\"\n",
    "        assert len(num_filters_list) == no_of_conv, \"The number of filters passes and number of convolutions do not match\"\n",
    "        assert len(max_pool_filter_size) == no_of_conv, \"The number of filters passes and number of convolutions do not match\"\n",
    "        assert len(max_pool_stride) == no_of_conv, \"The number of filters passes and number of convolutions do not match\"\n",
    "        \n",
    "        # Tracking the volume of the I/O\n",
    "        shape = [input_size, input_size, 3]\n",
    "\n",
    "        # Defining the convolution operational layers (Note default is stride 1, padding 0)\n",
    "        # CONV - 01\n",
    "        act_1 = return_activation_function(activation = activation_list[0])\n",
    "        if Batch_norm:\n",
    "            self.conv_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = 3, out_channels = num_filters_list[0], kernel_size = filter_sizes[0]),\n",
    "                nn.BatchNorm2d(C = 3),\n",
    "                act_1,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[0], stride = max_pool_stride[0])\n",
    "            )\n",
    "        else:\n",
    "            self.conv_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = 3, out_channels = num_filters_list[0], kernel_size = filter_sizes[0]),\n",
    "                act_1,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[0], stride = max_pool_stride[0])\n",
    "            )\n",
    "        \n",
    "        # Tracking the shape\n",
    "        shape[0] = np.ceil((shape[0] - filter_sizes[0] - 2*1)/1) + 1\n",
    "        shape[1] = np.ceil((shape[1] - filter_sizes[0] - 2*1)/1) + 1\n",
    "        shape[2] = num_filters_list[0]\n",
    "\n",
    "        # CONV - 02\n",
    "        act_2 = return_activation_function(activation = activation_list[1])\n",
    "        if Batch_norm:\n",
    "            self.conv_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[0], out_channels = num_filters_list[1], kernel_size = filter_sizes[1]),\n",
    "                nn.BatchNorm2d(C = num_filters_list[0]),\n",
    "                act_2,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[1], stride = max_pool_stride[1])\n",
    "            )\n",
    "        else:\n",
    "            self.conv_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[0], out_channels = num_filters_list[1], kernel_size = filter_sizes[1]),\n",
    "                act_2,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[1], stride = max_pool_stride[1])\n",
    "            )\n",
    "\n",
    "        # Tracking the shape\n",
    "        shape[0] = np.ceil((shape[0] - filter_sizes[1] - 2*1)/1) + 1\n",
    "        shape[1] = np.ceil((shape[1] - filter_sizes[1] - 2*1)/1) + 1\n",
    "        shape[2] = num_filters_list[1]\n",
    "\n",
    "        # CONV - 03\n",
    "        act_3 = return_activation_function(activation = activation_list[2])\n",
    "        if Batch_norm:\n",
    "            self.conv_3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[1], out_channels = num_filters_list[2], kernel_size = filter_sizes[2]),\n",
    "                nn.BatchNorm2d(C = num_filters_list[1]),\n",
    "                act_3,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[2], stride = max_pool_stride[2])\n",
    "            )\n",
    "        else:\n",
    "            self.conv_3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[1], out_channels = num_filters_list[2], kernel_size = filter_sizes[2]),\n",
    "                act_3,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[2], stride = max_pool_stride[2])\n",
    "            )\n",
    "\n",
    "        # Tracking the shape\n",
    "        shape[0] = np.ceil((shape[0] - filter_sizes[2] - 2*1)/1) + 1\n",
    "        shape[1] = np.ceil((shape[1] - filter_sizes[2] - 2*1)/1) + 1\n",
    "        shape[2] = num_filters_list[2]\n",
    "\n",
    "        # CONV - 04\n",
    "        act_4 = return_activation_function(activation = activation_list[3])\n",
    "        if Batch_norm:\n",
    "            self.conv_4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[2], out_channels = num_filters_list[3], kernel_size = filter_sizes[3]),\n",
    "                nn.BatchNorm2d(C = num_filters_list[2]),\n",
    "                act_4,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[3], stride = max_pool_stride[3])\n",
    "            )\n",
    "        else:\n",
    "            self.conv_4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[2], out_channels = num_filters_list[3], kernel_size = filter_sizes[3]),\n",
    "                act_4,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[3], stride = max_pool_stride[3])\n",
    "            )\n",
    "\n",
    "        # Tracking the shape\n",
    "        shape[0] = np.ceil((shape[0] - filter_sizes[3] - 2*1)/1) + 1\n",
    "        shape[1] = np.ceil((shape[1] - filter_sizes[3] - 2*1)/1) + 1\n",
    "        shape[2] = num_filters_list[3]\n",
    "\n",
    "        # CONV - 05\n",
    "        act_5 = return_activation_function(activation = activation_list[4])\n",
    "        if Batch_norm:\n",
    "            self.conv_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[3], out_channels = num_filters_list[4], kernel_size = filter_sizes[4]),\n",
    "                nn.BatchNorm2d(C = num_filters_list[3]),\n",
    "                act_5,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[4], stride = max_pool_stride[4])\n",
    "            )\n",
    "        else:\n",
    "            self.conv_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = num_filters_list[3], out_channels = num_filters_list[4], kernel_size = filter_sizes[4]),\n",
    "                act_5,\n",
    "                nn.MaxPool2d(kernel_size = max_pool_filter_size[4], stride = max_pool_stride[4])\n",
    "            )\n",
    "        \n",
    "        # Tracking the shape\n",
    "        shape[0] = np.ceil((shape[0] - filter_sizes[4] - 2*1)/1) + 1\n",
    "        shape[1] = np.ceil((shape[1] - filter_sizes[4] - 2*1)/1) + 1\n",
    "        shape[2] = num_filters_list[4]\n",
    "        \n",
    "        # FLATTEN\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # DENSE - 01\n",
    "        act_dense = return_activation_function(activation = dense_layer_activation)\n",
    "        self.dense_1 = nn.Sequential(\n",
    "            nn.Linear(in_features = shape[0]*shape[1]*shape[2], out_features = dense_layer_neurons),\n",
    "            act_dense,\n",
    "            )\n",
    "        \n",
    "        # DROPOUT \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # DENSE - 02 / OUTPUT\n",
    "        self.dense_out = nn.Sequential(\n",
    "            nn.Linear(in_features = dense_layer_neurons, out_features = n_classes),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        # Logging by W&B\n",
    "        self.save_hyperparameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
